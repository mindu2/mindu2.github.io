9월 24일
CS336 1강


강의 목표
Full understanding of this technology(system, data) is necessary for fundmental research
Gpt-4 1조 8천억 파라미터(1800B), 대신 이 강의에선 작은 모델을 만들 것 이다. Large 모델의 경우 작은 모델에서 나타나지 않는 현상들이 존재한다. 
1. MLP의 스케일에 관한 부분이고(최적화가 달라진다.).
2. Emergence ability 출현 이다 ([Wei et al., 2022](https://arxiv.org/abs/2206.07682)).
그럼에도 작은 모델을 시스템과 연결지어 구축 할 줄 안다는것과 직관을 기르는 것은 도움이 된다.

Pre-neural (before 2010s)

Language model to measure the entropy of English[Shannon 1950]
Lots of work on n-gram language models (for machine translation, speech recognition)[Brants+ 2007]

Neural ingredients (2010s)
    
First neural language model[Bengio+ 2003]    
Sequence-to-sequence modeling (for machine translation)[Sutskever+ 2014]    
Adam optimizer[Kingma+ 2014]
Attention mechanism (for machine translation)[Bahdanau+ 2014]
Transformer architecture (for machine translation)[Vaswani+ 2017]
Mixture of experts[Shazeer+ 2017]
Model parallelism[Huang+ 2018][Rajbhandari+ 2019][Shoeybi+ 2019]

Early foundation models (late 2010s)    
ELMo: pretraining with LSTMs, fine-tuning helps tasks[Peters+ 2018]
BERT: pretraining with Transformer, fine-tuning helps tasks[Devlin+ 2018]
Google's T5 (11B): cast everything as text-to-text[Raffel+ 2019]Embracing scaling, more closed    
OpenAI's GPT-2 (1.5B): fluent text, first signs of zero-shot, staged release
 [Radford+ 2019]    
Scaling laws: provide hope / predictability for scaling[Kaplan+ 2020]    
OpenAI's GPT-3 (175B): in-context learning, closed[Brown+ 2020]
Google's PaLM (540B): massive scale, undertrained[Chowdhery+ 2022]
DeepMind's Chinchilla (70B): compute-optimal scaling laws[Hoffmann+ 2022]

Open models    
EleutherAI's open datasets (The Pile) and models (GPT-J)[Gao+ 2020][Wang+ 2021] 
Meta's OPT (175B): GPT-3 replication, lots of hardware issues[Zhang+ 2022]
Hugging Face / BigScience's BLOOM: focused on data sourcing[Workshop+ 2022]
Meta's Llama models[Touvron+ 2023][Touvron+ 2023][Grattafiori+ 2024]
Alibaba's Qwen models[Qwen+ 2024]
DeepSeek's models[DeepSeek-AI+ 2024][DeepSeek-AI+ 2024][DeepSeek-AI+ 2024]    
AI2's OLMo 2[Groeneveld+ 2024][OLMo+ 2024]
    
Levels of openness    
Closed models (e.g., GPT-4o): API access only[OpenAI+ 2023]    
Open-weight models (e.g., DeepSeek): weights available, paper with architecture details, some training details, no data details[DeepSeek-AI+ 2024]
Open-source models (e.g., OLMo): weights and data available, paper with most details (but not necessarily the rationale, failed experiments)[Groeneveld+ 2024]
    
Today's frontier models    
OpenAI's o3
 https://openai.com/index/openai-o3-mini/
    
Anthropic's Claude Sonnet 3.7
 https://www.anthropic.com/news/claude-3-7-sonnet
    
xAI's Grok 3
 https://x.ai/news/grok-3
    
Google's Gemini 2.5
 https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/
    
Meta's Llama 3.3
 https://ai.meta.com/blog/meta-llama-3/
    
DeepSeek's r1
 [DeepSeek-AI+ 2025]
    
Alibaba's Qwen 2.5 Max
 https://qwenlm.github.io/blog/qwen2.5-max/
    
Tencent's Hunyuan-T1
 https://tencent.github.io/llm.hunyuan.T1/README_EN.html


Scaling Law
강의에서 Resources당 accuracy가 중요하다고 했다. 이에 조사한 결과  
