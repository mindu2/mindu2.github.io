# 강화학습


## 연구순서
<img width="2391" height="870" alt="image" src="https://github.com/user-attachments/assets/1e80583c-c137-4560-a429-301215dc7487" />

PPO제안 논문을 먼저 읽고, GRPO 제안 논문, Deepseek r1 논문 그리고 강화학습 코드를 통한 보상실험, Reinforcement Learning from Human Feedback를 통해서 레시피들을 참고할 예정이다.

======================================================

<img width="2322" height="1251" alt="image" src="https://github.com/user-attachments/assets/fa23b2df-ac8c-46a6-aba9-6c142eecf694" />

### 사전지식

우선 Reasoning을 길게 가져 갈려는 이유를 살펴보자, 왼쪽 이미지는 Openai에서 O1을 공개했을때 그래프이다. 저 당시 모델의 크기를 키우고 Train타임을 늘리면 성능이 증가한다는 사실은 일반적으로 알려져 있었다. 더불어서 Test Time또한 모델의 성능에 관여를 한다는 사실이 공개되었는데, 문제는 인퍼런스시간을 자율적으로 조절할 방법을 몰랐다. COT같은 경우 고정된 시퀀스의 SFT데이터로써 Test 타임을 모델에게 조절할 수 있는 방안은 아니였다. 오른쪽 그림은 허깅페이스의 블로그로 Test time을 조절 하기 위한 노력으로 같은 프롬프트에 대해 모델답변을 여러번 생성한 후 보팅하는 방법이다. k값의 증가에 따라 성능향상이 있으며 이 경우 또한 Test Time Scale과 관련이 있다. Test Time Scale을 위한 다른 시도로는 생성시 보상모델이 옆에서 점수를 매겨 높은 점수를 받은 토큰을 기준으로 이어서 생성하는 방식 또한 연구가 진행되었다.

어떻게 Test time을 조절할 수 있을까? 에 대한 답으로 Deepseek r1 논문이 나왔고 결론은 Cold Start + 강화학습 으로 현재는 알려져 있다. 
또 이 강화학습의 타켓으로는 일반적으로 STEM분야(Science, Technology, Engeener, Mathmatical)이며, Task들의 공통점은 문제의 난이도가 우연히 맞출 확률이 낮으며(찍어서 맞출 확률이 낮다.) 답이 명확하다.


<img width="2354" height="1221" alt="image" src="https://github.com/user-attachments/assets/f248fdbf-94da-4376-be7c-d6efe9205f4f" />

### 강화학습과 LLM
강화학습은 현재 정책일 때 보상의 기댓값이 최대를 목표로 한다.
Trajectory란 한 에피소드를 말한다. t-1시점의 상태 → t-1시점의 행동 → t-1시점의 보상 → t시점의 상태 → t시점의 행동 t시점의 보상  의 식으로 이루어진다. 이거를 LLM에 연결 한다. LLM은 토큰마다 확률을 내놓으며 시퀀스 출력을 Trajectroy로 볼 수 있다.  이때 현재 정책을 LLM자체로 보며 Score를 보상으로 해석해 보상의 최대값을 목표로 한다.

<img width="2367" height="1342" alt="image" src="https://github.com/user-attachments/assets/383e57b4-ed99-4a87-afab-7249ca0333e0" />
보상모델은 시퀀스 단위로 보상을 출력하는데 이를 이해하기위해 보상모델이 학습하는 방식을 살펴보자. 사진은 보상모델의 학습방식이다
보상모델은 기본적으로 두 문장씩 대조학습을 통해 더 나은 문장을 구분하여 Score를 출력한다. 학습단위가 문장이기 때문에 보상모델의 출력은 문장단위임을 알 수 있다.
LLM의 생성에 대해 보상모델이 출력하는 값은 시퀀스단위로 한 문장의 토큰들은 모두 같은 보상값을 가진다(그 문장의 보상값). 따라서 토큰단위의 보상인 Value를 예측하는 과정이 필요하다.

<img width="2333" height="1294" alt="image" src="https://github.com/user-attachments/assets/e94fa9be-4140-4f3e-939d-1f30b4c7aa97" />
그림에서 보듯이 학습이 완료된 보상모델을 두번 사용한다. 하나는 흔히 알고있는 Reward 모델이며, 하나는 Value Function으로 토큰당 보상을 예측한다. 초기 두 파라미터 값은 같지만 Value function의 경우 LLM을 강화학습하면서 함께 학습된다.
이때 Value Function의 학습방식은 회귀이다.

<img width="2257" height="1276" alt="image" src="https://github.com/user-attachments/assets/b1eed0b6-080f-4806-b3db-25ee493b356b" />
GRPO는 여기서 Value Function대신 모델의 생성응답들을 활용한다. Value function이 내놓은 토큰당 예측 보상값 대신 생성된 답변끼리의 Reward평균값을 통해 R - Value를 R - mean(r) 로 대채해 Value function의 존재를 없앤다.

<img width="2245" height="1240" alt="image" src="https://github.com/user-attachments/assets/f63cd4f7-521c-4971-81d8-505c9c35adab" />
앞선 선행연구들을 통해 알 수 있는 공통점은 언어모델의 강화학습을 할 때, 아래 3가지만 지키면 된다.
1. 무엇이 더 낫다라는 데이터)
2. KL 디버젼시 와 같은 일종의 규제(L2 Norm처럼 규제), 실제로 구현은 (학습 후 모델의 생성한 토큰에대한 Reward값)을 (학습 전 모델의 생성한 Reward값)으로 나누기만 해서 규제한다.
3. Reward(모델이 Reward를 줄 경우 보상모델이 필요하지만, Deepseek 처럼 Rule Base일 경우 보상모델 또한 필요없다.)
이 3가지만 지키면 되기에 언어모델의 다양한 강화학습 알고리즘이 계속 나올 수 있다.
=================================================================



## DeepSeekMath: Pushing the Limits of Mathematical
Reasoning in Open Language Models — GRPO


fastText-based classifier (Joulin et al.,2016). — 텍스트 분류기 

tool-integrated reasoning (Gou et al., 2023) data — 2023 년도에 agent

데이터수집 : 크롤링(근거있게 크롤링) →DeepSeekMath Corpus구축
수집한 데이터가 괜찮은지 평가 하기 위해 
MatPile(8.9B), Proof-Pile-2(52B), OpenWebMath(13.6B), DeepSeekMath Corpus(120B)를
DeepSeek-LLM 1.3B모델에 대해 학습 후 평가 진행(모든 데이터 수는 150B로 맞춤,중복 샘플링 한 거 같음)
DeepSeekMath Corpus 성능이 가장 좋았다고 함

PT단계
DeepSeek-Coder-Base-v1.5 7B 모델 500B 데이터 PT(DeepSeekMath Corpus, Arxiv, AlgebraStack, Github, 일반 CC데이터)

SFT단계
GSM8K, MATH, Lila-OOD, Chinese K-12 mathematical 을 이용해 776k 수학 데이터 구축
3가지 유형으로 구축 COT(사고과정이 글로 적힌 데이터), POT(사고과정에 수식적인 계산이 적힌 데이터), Tool(COT, POT혼합)
여기까지 해서 이미 잘했다고 함

![image.png](https://mindu2.github.io/blog/images/deepseekmath_eval.png)

그림은 마지막 두줄 보면 됨
PPO

![image.png](https://mindu2.github.io/blog/images/deepseekmath_ppo.png)

cf) 보상모델의 경우 데이터 Rank에 따른 점수 간격을 최대화하게 학습 됨 따라서 특정 기준을 가진 분포가 아니며 출력 값이 실수이다. 따라서 정규화 한 값 A를 사용한다 ㅡ A는 (보상 - 보상예측값) 이다, 앞서 학습 완료된 보상 모델이 있다. 이것을 두 번 사용한다. 하나는 파라미터를 고정해서 스코어를 측정 하는 용도, 하나는 LLM을 PPO하면서 같이 학습 되어 보상 기댓값을 예측 하는 용도, 여기서 학습되는 Reward Model을 Value function이라 하며 보상 예측 값은 Value Function의 Output이다. Value funtion은 회귀로 (실제보상- 예측값)^2으로 학습되 단순히 이 시점에서 이 보상이 끝까지 갈 경우 최종 보상 값을 예측한다. 그러면 매시점 모델이 지금 생성이 괜찮은지를 판단할 수 있다. 이렇게 설계된 이유는 reward 모델은 샘플 단위로 학습해 한 샘플에 대한 score를 출력하기 때문이다. 그 결과로  문장 내 모든 토큰 들은 모두 같은 보상 R을 가지게 된다. 이 부분은 학습 시 샘플이 주어진 순간 상수 R값을 가진다고 이해하면 된다. 하지만 PPO는 토큰 단위의 업데이트를 설계한다. 토큰 단위의 업데이트를 위해 중간 토큰들에 대한 값 V가 필요하고 이를 위해 Value Function으로 (지금까지 이런 토큰들이 나왔을때 이 뒤로 끝까지 생성하면 평균적으로 받을 최종)보상을 출력한다. 그리고 A=R-V를 구해 업데이트를 진행한다.
질문 분포 P 중 하나인 Query에 대해서 생성한 답변에 대해 A를 계산, 
A는 (보상 - BaseLine)값으로 모델이 토큰을 출력하는 순간 정해지며 정규화된 보상
또한 위 식은 기존 PPO에서  KL제한이 빠진 것 처럼 보이나

![image.png](https://mindu2.github.io/blog/images/kl_divergency.png)

항을 곱해서 역할을 대신 함,  
clip을 적용한 것과 비교해 작은 값을 선택 및 gradient 업데이트를 진행
GRPO는 여기서 A(실제 보상 - 예측 보상 기댓값)정규화 방식을 바꾸자 주장한다.
1. Value funtion을 학습하면서 연산량이 증가된다. 따라서 Value model 대신, 같은 질문에서 나온 답변들끼리 서로 비교하자
2. A에 곱하던 KL을 곱하지 말고 빼자 
3. 

GRPO

![image.png](https://mindu2.github.io/blog/images/deepseekmath_grpo.png)

=================================================================


### Training language models to follow instructions
with human feedback — Openai PPO 제안논문

데이터 실험 규모 
SFT 13K
RL 33K


### 순서



 GPT3 이 있음 → 사람 40명이 SFT데이터 수집 → 수집된 데이터로 GPT3-SFT 16epoch학습 → k=4 로 해서 답변 4개씩 생성 → 사람 40명이 4개의 순서(rank)를 매김 → 이 데이터로 보상 모델 학습 → RLHF학습을 위한 초기화 용 GPT3-SFT를 새로 만들기  (2 epoch, 생성된 SFT데이터 + PT데이터 10%추가) → 이렇게 생성된 GPT3-SFT(앞선 16에폭 SFT랑 다른것으로 16epoch SFT모델은 보상모델을 위한 데이터를 만들기 위해 구운것이다.)에 대해 query를 날리고 생성한 답변에 대해 보상모델이 점수를 측정 → 이 점수로 학습 진행

보상모델  오브젝트 함수 

![image.png](https://mindu2.github.io/blog/images/ppo_loss.png)

x라는 프롬프트에 대해 yw와 yl을 출력하는 분포에 대해서 보상모델이 내놓은 yw의 점수와 yl점수간의 차이가 커지도록 학습

PPO-ptx 오브젝트 함수(ptx는 모델의 로스를 의미), 참고 PPO는 토큰단위 계산

2015년에 TRPO라고 나왔는 강화학습에서 Policy 업데이트를 안정적으로 하기 위해 hard한 KL제약을 걸음2017년에  Openai는 TRPO의 KL제약이 연산량의 증가와 구현의 복잡성으로 답이 없음에 KL대신 현재 업데이트에서 이전업데이트와 큰 차이가 없으면 되 라는 조건을 걸고 구현은 (업데이트 모델의 토큰 확률/기존 모델의 토큰 확률)로 진행한다.

![image.png](https://mindu2.github.io/blog/images/ppo_object.png)

x에 대해 y를 output으로 출력하는 현재 학습대상인 LLM에 대해
1.  보상모델이 측정한 점수(실제로는 socre 정규화 값으로 r세타가 아닌 A를 사용)
2. RL학습전 모델의 분포와 학습 중인 모델의 파라미터 비 (KL은 답이없어서 대용)
3. 현재 모델이 입력 토큰에 대한 로스
를 더한 값
기존 분포 제약을  건 이유 : 보상모델의 결과는 실수값 (-무한, 무한)이므로 제한요소가 필요 
추가로 실험을 통해 Openai는 현재 쿼리에대한 모델의 로스또한 포함한  ppo-ptx가  결과가 좋다 하였음

SFT학습 arg
epoch 16, dropout 0.2, 코사인스케줄러사용 warm up없이 초기의 10% 수치까지 decay
6B모델의 보상모델이 가장 좋았다고 함

보상모델 arg
epoch 1 중요, lr : 9e-6, batch 64

PPO arg
batch 512개로 오브젝트 계산에 필요한 값들을 수집
수집한 값들로 64개씩 minibatch로 업데이트
ppo clip 0.2
베타 값 0.02
EMA 0.992 (실제 파라미터 업데이트시 EMA으로 업데이트)

=================================================================


### DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning


lr 3e-6
kl 베타값 : 0.001
temperature 1
16개씩 샘플링

reward 는 <think></think><answer></answer> 포멧에 대한 점수 + 정답여부

![image.png](https://mindu2.github.io/blog/images/deepseekr1_outline.png)


그림하고 본문 적힌 내용이 조금 다름, 본문 내용을 아래 정리함

1. Deep Seek V3 → rl : 좀 이상함, reasonin은 잘하는데 언어가 섞이고 대화처럼 보이진 않음 → 데이터 합성 후 필터링
2. 1번데이터로 Deep Seek V3 SFT(이게 Cold Start라고 말하는거) → 1번 RL → SFT → 2번 RL

1번 RL : Reasoning강화용, 포맷보상 + 정답보상 + Language보상(언어 하나만 썼는지)
RL 사이 SFT : 1번RL 까지 학습 한 모델로 데이터 합성 → 필터링 후 Rejection 샘플을 만들고 일반 데이터들을 추가해서 한번더 SFT (일반 말하기 능력 회복)
2번 RL : Preference RL 단계로 다양한 프롬프트에 대해 (reasoning, general 문제 섞어서)
식
Reward  = Reward(Reasoning) + Reward(General) + Reward(Language)
Reward(Reasoning)  - Format(포맷 내용 X, 포맷의 여부만 점수), Answer
Reward(General) - Format, Reward_Model
순서
Reward(Reasoning) 하고나서 일반 Instruction 데이터(General) + Preference reward(Reward(General)이라고 이해 했음, 요약 글쓰기 이런거 보상모델로 점수 준 듯) 400step

→한국어만 생성하도록 보상줄 수 있을 듯

---

=================================================================


### Defeating the Training-Inference Mismatch via FP16


강화학습할때 bf16대신 fp16 쓰면 그동안 RL할때 문제되었던 부분인 안정적으로 학습 안 되던거 해결되고 성능도 올라 라는데→ 실험 해봐야 함

=================================================================

### TRL 라이브러리

[https://huggingface.co/docs/trl/grpo_trainerhttps://huggingface.co/docs/trl/grpo_trainer](https://huggingface.co/docs/trl/grpo_trainer)

참고
