# 강화학습


## 연구순서
<img width="2391" height="870" alt="image" src="https://github.com/user-attachments/assets/1e80583c-c137-4560-a429-301215dc7487" />

PPO제안 논문을 먼저 읽고, GRPO 제안 논문, Deepseek r1 논문 그리고 강화학습 코드를 통한 보상실험, Reinforcement Learning from Human Feedback를 통해서 레시피들을 참고할 예정이다.

======================================================

<img width="2322" height="1251" alt="image" src="https://github.com/user-attachments/assets/fa23b2df-ac8c-46a6-aba9-6c142eecf694" />

### 사전지식

우선 Reasoning을 길게 가져 갈려는 이유를 살펴보자, 왼쪽 이미지는 Openai에서 O1을 공개했을때 그래프이다. 저 당시 모델의 크기를 키우고 Train타임을 늘리면 성능이 증가한다는 사실은 일반적으로 알려져 있었다. 더불어서 Test Time또한 모델의 성능에 관여를 한다는 사실이 공개되었는데, 문제는 인퍼런스시간을 자율적으로 조절할 방법을 몰랐다. COT같은 경우 고정된 시퀀스의 SFT데이터로써 Test 타임을 모델에게 조절할 수 있는 방안은 아니였다. 오른쪽 그림은 허깅페이스의 블로그로 Test time을 조절 하기 위한 노력으로 같은 프롬프트에 대해 모델답변을 여러번 생성한 후 보팅하는 방법이다. k값의 증가에 따라 성능향상이 있으며 이 경우 또한 Test Time Scale과 관련이 있다. Test Time Scale을 위한 다른 시도로는 생성시 보상모델이 옆에서 점수를 매겨 높은 점수를 받은 토큰을 기준으로 이어서 생성하는 방식 또한 연구가 진행되었다.

어떻게 Test time을 조절할 수 있을까? 에 대한 답으로 Deepseek r1 논문이 나왔고 결론은 Cold Start + 강화학습 으로 현재는 알려져 있다. 
또 이 강화학습의 타켓으로는 일반적으로 STEM분야(Science, Technology, Engeener, Mathmatical)이며, Task들의 공통점은 문제의 난이도가 우연히 맞출 확률이 낮으며(찍어서 맞출 확률이 낮다.) 답이 명확하다.


<img width="2354" height="1221" alt="image" src="https://github.com/user-attachments/assets/f248fdbf-94da-4376-be7c-d6efe9205f4f" />

### 강화학습과 LLM
강화학습은 현재 정책일 때 보상의 기댓값이 최대를 목표로 한다.
Trajectory란 한 에피소드를 말한다. t-1시점의 상태 → t-1시점의 행동 → t-1시점의 보상 → t시점의 상태 → t시점의 행동 t시점의 보상  의 식으로 이루어진다. 이거를 LLM에 연결 한다. LLM은 토큰마다 확률을 내놓으며 시퀀스 출력을 Trajectroy로 볼 수 있다.  이때 현재 정책을 LLM자체로 보며 Score를 보상으로 해석해 보상의 최대값을 목표로 한다.

<img width="2367" height="1342" alt="image" src="https://github.com/user-attachments/assets/383e57b4-ed99-4a87-afab-7249ca0333e0" />
보상모델은 시퀀스 단위로 보상을 출력하는데 이를 이해하기위해 보상모델이 학습하는 방식을 살펴보자. 사진은 보상모델의 학습방식이다
보상모델은 기본적으로 두 문장씩 대조학습을 통해 더 나은 문장을 구분하여 Score를 출력한다. 학습단위가 문장이기 때문에 보상모델의 출력은 문장단위임을 알 수 있다.
LLM의 생성에 대해 보상모델이 출력하는 값은 시퀀스단위로 한 문장의 토큰들은 모두 같은 보상값을 가진다(그 문장의 보상값). 따라서 토큰단위의 보상인 Value를 예측하는 과정이 필요하다.

<img width="2333" height="1294" alt="image" src="https://github.com/user-attachments/assets/e94fa9be-4140-4f3e-939d-1f30b4c7aa97" />
그림에서 보듯이 학습이 완료된 보상모델을 두번 사용한다. 하나는 흔히 알고있는 Reward 모델이며, 하나는 Value Function으로 토큰당 보상을 예측한다. 초기 두 파라미터 값은 같지만 Value function의 경우 LLM을 강화학습하면서 함께 학습된다.
이때 Value Function의 학습방식은 회귀이다.

<img width="2257" height="1276" alt="image" src="https://github.com/user-attachments/assets/b1eed0b6-080f-4806-b3db-25ee493b356b" />
GRPO는 여기서 Value Function대신 모델의 생성응답들을 활용한다. Value function이 내놓은 토큰당 예측 보상값 대신 생성된 답변끼리의 Reward평균값을 통해 R - Value를 R - mean(r) 로 대채해 Value function의 존재를 없앤다.

<img width="2245" height="1240" alt="image" src="https://github.com/user-attachments/assets/f63cd4f7-521c-4971-81d8-505c9c35adab" />
앞선 선행연구들을 통해 언어모델을 강화학습을 할 때, 아래 3가지만 지키면 된다는 것을 알 수 있다.

1. 무엇이 더 낫다라는 데이터)
2. KL 디버젼시 와 같은 일종의 규제(L2 Norm처럼 규제), 실제로 구현은 (학습 후 모델의 생성한 토큰에대한 Reward값)을 (학습 전 모델의 생성한 Reward값)으로 나누기만 해서 규제한다.
3. Reward(모델이 Reward를 줄 경우 보상모델이 필요하지만, Deepseek 처럼 Rule Base일 경우 보상모델 또한 필요없다.)
   
이 3가지만 지키면 되기에 언어모델의 다양한 강화학습 알고리즘이 계속 나오는 것 같다.


=================================================================


### Defeating the Training-Inference Mismatch via FP16


강화학습할때 bf16대신 fp16 쓰면 그동안 RL할때 문제되었던 부분인 안정적으로 학습 안 되던거 해결되고 성능도 올라 라는데→ 실험 해봐야 함

=================================================================

### TRL 라이브러리

[https://huggingface.co/docs/trl/grpo_trainerhttps://huggingface.co/docs/trl/grpo_trainer](https://huggingface.co/docs/trl/grpo_trainer)

참고
