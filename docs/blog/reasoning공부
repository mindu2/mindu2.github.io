# 강화학습

### 연구순서

Training language models to follow instructions with human feedback(PPO) → DeepSeekMath → DeepSeek R1 → TRL → 보상 실험 → 서베이 Reinforcement Learning from Human Feedback
A short introduction to RLHF and post-training focused on language models → 학습 방식 실험

=================================================================

### 사전지식

Reasoning을 길게 가져 갈려는 이유  → Test time을 늘리면 성능이 증가함 
STEM분야(Science, Technology, Engeener, Mathmatical)에 대해 강화학습 하는 이유 → STEM분야를 하면 Reasoning 능력이 대체로 좋아지는 경향, Task들의 공통점은 문제의 난이도가 우연히 맞출 확률이 낮으며 답이 명확함
어떻게 Test time을 조절할 수 있을까? 이걸 deepseek r1논문 참고  → 결론 : Cold Start + 강화학습 

강화학습은 현재 정책일 때 보상의 기댓값이 최대를 목표로 한다.
Trajectory란 한 에피소드를 말한다. t-1시점의 상태 → t-1시점의 행동 → t-1시점의 보상 → t시점의 상태 → t시점의 행동 t시점의 보상  의 식으로 이루어진다. 이거를 LLM에 연결 하는 것 이다. LLM은 토큰마다 확률을 내놓으며 시퀀스 출력을 Trajectroy로 볼 수 있다.  이때 현재 정책을 LLM자체로 보며 Score를 보상으로 해석해 보상의 최대값을 목표로 한다. 미분을 하게 되면 수식에서 현재 시점의 보상 값이 그라디언트에 그대로 곱해져 값이 커진다. 원하는 건 그라디언트이기 때문에 기대 예측값을 빼 조정한다. LLM강화학습 논문을 보면 r(x,y) 대신 등장하는 (x,y)A 값은 정규화된 보상이다. 

2015년에 TRPO라고 나왔는 강화학습에서 Policy 업데이트를 안정적으로 하기 위해 hard한 KL제약을 걸음2017년에  Openai는 TRPO의 KL제약이 연산량의 증가와 구현의 복잡성으로 답이 없음에 KL대신 현재 업데이트에서 이전업데이트와 큰 차이가 없으면 되 라는 조건을 걸고 구현은 (업데이트 모델의 토큰 확률/기존 모델의 토큰 확률)로 진행한다.

=================================================================

### 강화학습 결론

무엇이 더 낫다 라는 데이터만 있으면 됨, KL처럼 큰 변형이 없도록 제한만 있으면 됨 
PPO는 Reward 모델이 필요함, 왜냐하면 Reward 측정용, Value 측정용 둘다 필요
Grpo의 경우 Value측정용 Reward 모델이 필요없음 왜냐하면 생성된 출력 끼리 비교할꺼라서
더 나아가서 Grpo에서 Reward를 줄 수 있는 뭔가 만 있으면 Reward 모델 도 필요없음
Deepseek r1은 맞다 틀렸다만 Reward로 주면 되니까 Reward모델도 필요없음(써도 됨)

=================================================================

## DeepSeekMath: Pushing the Limits of Mathematical
Reasoning in Open Language Models — GRPO

fastText-based classifier (Joulin et al.,2016). — 텍스트 분류기 

tool-integrated reasoning (Gou et al., 2023) data — 2023 년도에 agent

데이터수집 : 크롤링(근거있게 크롤링) →DeepSeekMath Corpus구축
수집한 데이터가 괜찮은지 평가 하기 위해 
MatPile(8.9B), Proof-Pile-2(52B), OpenWebMath(13.6B), DeepSeekMath Corpus(120B)를
DeepSeek-LLM 1.3B모델에 대해 학습 후 평가 진행(모든 데이터 수는 150B로 맞춤,중복 샘플링 한 거 같음)
DeepSeekMath Corpus 성능이 가장 좋았다고 함

PT단계
DeepSeek-Coder-Base-v1.5 7B 모델 500B 데이터 PT(DeepSeekMath Corpus, Arxiv, AlgebraStack, Github, 일반 CC데이터)

SFT단계
GSM8K, MATH, Lila-OOD, Chinese K-12 mathematical 을 이용해 776k 수학 데이터 구축
3가지 유형으로 구축 COT(사고과정이 글로 적힌 데이터), POT(사고과정에 수식적인 계산이 적힌 데이터), Tool(COT, POT혼합)
여기까지 해서 이미 잘했다고 함

![image.png](attachment:69e4cbef-b33f-4cbc-852e-7104bb3ec618:image.png)

그림은 마지막 두줄 보면 됨
PPO

![image.png](attachment:6cfde2de-f7e6-40a4-8e25-34e741b9ee1f:image.png)

cf) 보상모델의 경우 데이터 Rank에 따른 점수 간격을 최대화하게 학습 됨 따라서 특정 기준을 가진 분포가 아니며 출력 값이 실수이다. 따라서 정규화 한 값 A를 사용한다 ㅡ A는 (보상 - 보상예측값) 이다, 앞서 학습 완료된 보상 모델이 있다. 이것을 두 번 사용한다. 하나는 파라미터를 고정해서 스코어를 측정 하는 용도, 하나는 LLM을 PPO하면서 같이 학습 되어 보상 기댓값을 예측 하는 용도, 여기서 학습되는 Reward Model을 Value function이라 하며 보상 예측 값은 Value Function의 Output이다. Value funtion은 회귀로 (실제보상- 예측값)^2으로 학습되 단순히 이 시점에서 이 보상이 끝까지 갈 경우 최종 보상 값을 예측한다. 그러면 매시점 모델이 지금 생성이 괜찮은지를 판단할 수 있다. 이렇게 설계된 이유는 reward 모델은 샘플 단위로 학습해 한 샘플에 대한 score를 출력하기 때문이다. 그 결과로  문장 내 모든 토큰 들은 모두 같은 보상 R을 가지게 된다. 이 부분은 학습 시 샘플이 주어진 순간 상수 R값을 가진다고 이해하면 된다. 하지만 PPO는 토큰 단위의 업데이트를 설계한다. 토큰 단위의 업데이트를 위해 중간 토큰들에 대한 값 V가 필요하고 이를 위해 Value Function으로 (지금까지 이런 토큰들이 나왔을때 이 뒤로 끝까지 생성하면 평균적으로 받을 최종)보상을 출력한다. 그리고 A=R-V를 구해 업데이트를 진행한다.
질문 분포 P 중 하나인 Query에 대해서 생성한 답변에 대해 A를 계산, 
A는 (보상 - BaseLine)값으로 모델이 토큰을 출력하는 순간 정해지며 정규화된 보상
또한 위 식은 기존 PPO에서  KL제한이 빠진 것 처럼 보이나

![image.png](attachment:b34c2a82-03ba-4759-9485-e8efc0367f6d:image.png)

항을 곱해서 역할을 대신 함,  
clip을 적용한 것과 비교해 작은 값을 선택 및 gradient 업데이트를 진행
GRPO는 여기서 A(실제 보상 - 예측 보상 기댓값)정규화 방식을 바꾸자 주장한다.
1. Value funtion을 학습하면서 연산량이 증가된다. 따라서 Value model 대신, 같은 질문에서 나온 답변들끼리 서로 비교하자
2. A에 곱하던 KL을 곱하지 말고 빼자 
3. 

GRPO

![image.png](attachment:7fdecdb7-04b5-4166-8d59-19a5ca810669:image.png)

=================================================================

### Training language models to follow instructions
with human feedback — Openai PPO 제안논문

데이터 실험 규모 
SFT 13K
RL 33K

### 순서

 GPT3 이 있음 → 사람 40명이 SFT데이터 수집 → 수집된 데이터로 GPT3-SFT 16epoch학습 → k=4 로 해서 답변 4개씩 생성 → 사람 40명이 4개의 순서(rank)를 매김 → 이 데이터로 보상 모델 학습 → RLHF학습을 위한 초기화 용 GPT3-SFT를 새로 만들기  (2 epoch, 생성된 SFT데이터 + PT데이터 10%추가) → 이렇게 생성된 GPT3-SFT(앞선 16에폭 SFT랑 다른것으로 16epoch SFT모델은 보상모델을 위한 데이터를 만들기 위해 구운것이다.)에 대해 query를 날리고 생성한 답변에 대해 보상모델이 점수를 측정 → 이 점수로 학습 진행

보상모델  오브젝트 함수 

![image.png](attachment:4a114489-965a-478a-924a-3d950421d23a:image.png)

x라는 프롬프트에 대해 yw와 yl을 출력하는 분포에 대해서 보상모델이 내놓은 yw의 점수와 yl점수간의 차이가 커지도록 학습

PPO-ptx 오브젝트 함수(ptx는 모델의 로스를 의미), 참고 PPO는 토큰단위 계산

![image.png](attachment:1a5765fe-3e4a-4c0e-b47c-f830a4fec640:image.png)

x에 대해 y를 output으로 출력하는 현재 학습대상인 LLM에 대해
1.  보상모델이 측정한 점수(실제로는 socre 정규화 값으로 r세타가 아닌 A를 사용)
2. RL학습전 모델의 분포와 학습 중인 모델의 파라미터 비 (KL은 답이없어서 대용)
3. 현재 모델이 입력 토큰에 대한 로스
를 더한 값
기존 분포 제약을  건 이유 : 보상모델의 결과는 실수값 (-무한, 무한)이므로 제한요소가 필요 
추가로 실험을 통해 Openai는 현재 쿼리에대한 모델의 로스또한 포함한  ppo-ptx가  결과가 좋다 하였음

SFT학습 arg
epoch 16, dropout 0.2, 코사인스케줄러사용 warm up없이 초기의 10% 수치까지 decay
6B모델의 보상모델이 가장 좋았다고 함

보상모델 arg
epoch 1 중요, lr : 9e-6, batch 64

PPO arg
batch 512개로 오브젝트 계산에 필요한 값들을 수집
수집한 값들로 64개씩 minibatch로 업데이트
ppo clip 0.2
베타 값 0.02
EMA 0.992 (실제 파라미터 업데이트시 EMA으로 업데이트)

=================================================================

### DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

lr 3e-6
kl 베타값 : 0.001
temperature 1
16개씩 샘플링

reward 는 <think></think><answer></answer> 포멧에 대한 점수 + 정답여부

![image.png](attachment:1b68b8a2-54b5-431c-a2d5-7ef86fb84ac5:image.png)

그림하고 본문 적힌 내용이 조금 다름, 본문 내용을 아래 정리함

1. Deep Seek V3 → rl : 좀 이상함, reasonin은 잘하는데 언어가 섞이고 대화처럼 보이진 않음 → 데이터 합성 후 필터링
2. 1번데이터로 Deep Seek V3 SFT(이게 Cold Start라고 말하는거) → 1번 RL → SFT → 2번 RL

1번 RL : Reasoning강화용, 포맷보상 + 정답보상 + Language보상(언어 하나만 썼는지)
RL 사이 SFT : 1번RL 까지 학습 한 모델로 데이터 합성 → 필터링 후 Rejection 샘플을 만들고 일반 데이터들을 추가해서 한번더 SFT (일반 말하기 능력 회복)
2번 RL : Preference RL 단계로 다양한 프롬프트에 대해 (reasoning, general 문제 섞어서)
식
Reward  = Reward(Reasoning) + Reward(General) + Reward(Language)
Reward(Reasoning)  - Format(포맷 내용 X, 포맷의 여부만 점수), Answer
Reward(General) - Format, Reward_Model
순서
Reward(Reasoning) 하고나서 일반 Instruction 데이터(General) + Preference reward(Reward(General)이라고 이해 했음, 요약 글쓰기 이런거 보상모델로 점수 준 듯) 400step

→한국어만 생성하도록 보상줄 수 있을 듯

---

=================================================================

### Defeating the Training-Inference Mismatch via FP16

강화학습할때 bf16대신 fp16 쓰면 그동안 RL할때 문제되었던 부분인 안정적으로 학습 안 되던거 해결되고 성능도 올라 라는데→ 실험 해봐야 함

=================================================================

### TRL 라이브러리

[https://huggingface.co/docs/trl/grpo_trainerhttps://huggingface.co/docs/trl/grpo_trainer](https://huggingface.co/docs/trl/grpo_trainer)

참고

=================================================================
Reinforcement Learning from Human Feedback
A short introduction to RLHF and post-training focused on language models

=================================================================

